# Sign Language Detection  

This project focuses on **real-time Sign Language Detection** using **MediaPipe** for hand tracking and a deep learning model for gesture classification. It includes **data collection, model training, and real-time inference**.

---

## Features  
âœ” **Real-time hand tracking** using MediaPipe  
âœ” **Sign gesture classification** with deep learning  
âœ” **Dataset collection & preprocessing**  
âœ” **Visualization tools** for evaluation  

---

## Usage
1ï¸ **Collect Data (Optional)**
Use data-collection.ipynb to capture and store hand gesture data.
2ï¸âƒ£ **Train the Model*
Run sign-language-detection.ipynb for model training, which includes:
    LSTMs & CNNs for feature extraction.
    TensorFlow/Keras for implementation.
3ï¸âƒ£ Run the Detection System
Execute the detection notebook to test real-time sign recognition.

---

## Project Structure

ğŸ“ code/ â€“ Jupyter notebooks for data collection & detection
ğŸ“ figures/ â€“ Visuals & model architecture images
ğŸ“ gifs/ â€“ Demonstration GIFs
