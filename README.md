# Sign Language Detection  

This project focuses on **real-time Sign Language Detection** using **MediaPipe** for hand tracking and a deep learning model for gesture classification. It includes **data collection, model training, and real-time inference**.

---

## Features  
✔ **Real-time hand tracking** using MediaPipe  
✔ **Sign gesture classification** with deep learning  
✔ **Dataset collection & preprocessing**  
✔ **Visualization tools** for evaluation  

---

## Usage
1️ **Collect Data (Optional)**
Use data-collection.ipynb to capture and store hand gesture data.
2️⃣ **Train the Model*
Run sign-language-detection.ipynb for model training, which includes:
    LSTMs & CNNs for feature extraction.
    TensorFlow/Keras for implementation.
3️⃣ Run the Detection System
Execute the detection notebook to test real-time sign recognition.

---

## Project Structure

📁 code/ – Jupyter notebooks for data collection & detection
📁 figures/ – Visuals & model architecture images
📁 gifs/ – Demonstration GIFs
